{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "APP_NAME=\"acccident_data_process\"\n",
    "from pyspark.sql import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder.master(\"local\").appName(\"Word Count\")\\\n",
    "#                 .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "#                 .getOrCreate()\n",
    "sc=SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext=SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "accident_data=sqlContext.read.csv(\"accident_data_merged.csv\",\n",
    "                                  header=True,encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- _c0: string (nullable = true)\n |-- Unnamed: 0: string (nullable = true)\n |-- accidenttime: string (nullable = true)\n |-- accidentaddr: string (nullable = true)\n |-- driver1fault: string (nullable = true)\n |-- driver1responsibility: string (nullable = true)\n |-- driver2responsibility: string (nullable = true)\n |-- sex1: string (nullable = true)\n |-- platenumber1: string (nullable = true)\n |-- carcolor1: string (nullable = true)\n |-- sex2: string (nullable = true)\n |-- platenumber2: string (nullable = true)\n |-- carcolor2: string (nullable = true)\n |-- jxmc1: string (nullable = true)\n |-- jxmc2: string (nullable = true)\n |-- cclzrq1: string (nullable = true)\n |-- cclzrq2: string (nullable = true)\n |-- clpp1: string (nullable = true)\n |-- clpp2: string (nullable = true)\n |-- driver1license: string (nullable = true)\n |-- driver2license: string (nullable = true)\n |-- brith1: string (nullable = true)\n |-- brith2: string (nullable = true)\n |-- accident_month: string (nullable = true)\n |-- accident_quarter: string (nullable = true)\n |-- accident_weekday: string (nullable = true)\n |-- accident_day: string (nullable = true)\n |-- accident_hour: string (nullable = true)\n |-- accident_minute: string (nullable = true)\n |-- is_province1: string (nullable = true)\n |-- is_province2: string (nullable = true)\n |-- is_city1: string (nullable = true)\n |-- is_city2: string (nullable = true)\n |-- is_private: string (nullable = true)\n |-- end_number1: string (nullable = true)\n |-- end_number2: string (nullable = true)\n |-- is_private1: string (nullable = true)\n |-- is_private2: string (nullable = true)\n |-- is_driver1_city: string (nullable = true)\n |-- is_driver2_city: string (nullable = true)\n |-- is_driver1_province: string (nullable = true)\n |-- is_driver2_province: string (nullable = true)\n |-- driver1_days: string (nullable = true)\n |-- driver2_days: string (nullable = true)\n |-- driver1_years: string (nullable = true)\n |-- driver2_years: string (nullable = true)\n |-- driver1_age: string (nullable = true)\n |-- driver2_age: string (nullable = true)\n |-- difference_age: string (nullable = true)\n |-- clcpp1: string (nullable = true)\n |-- clcpp2: string (nullable = true)\n |-- id_x: string (nullable = true)\n |-- temperature_min: string (nullable = true)\n |-- temperature_max: string (nullable = true)\n |-- weather1: string (nullable = true)\n |-- weather2: string (nullable = true)\n |-- wind1: string (nullable = true)\n |-- wind2: string (nullable = true)\n |-- diff_temperature: string (nullable = true)\n |-- district: string (nullable = true)\n |-- lng: string (nullable = true)\n |-- lat: string (nullable = true)\n |-- id_y: string (nullable = true)\n |-- jszh_x: string (nullable = true)\n |-- fine_x: string (nullable = true)\n |-- score_x: string (nullable = true)\n |-- maxtime_x: string (nullable = true)\n |-- xfcount_x: string (nullable = true)\n |-- wfxw_x: string (nullable = true)\n |-- id: string (nullable = true)\n |-- jszh_y: string (nullable = true)\n |-- fine_y: string (nullable = true)\n |-- score_y: string (nullable = true)\n |-- maxtime_y: string (nullable = true)\n |-- xfcount_y: string (nullable = true)\n |-- wfxw_y: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "accident_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "accident_data=accident_data.drop(\"Unnamed: 0\")\n",
    "accident_data=accident_data.drop(\"_c0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "accident_data=accident_data.drop(\"id_x\")\n",
    "accident_data=accident_data.drop(\"id_y\")\n",
    "accident_data=accident_data.drop(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "accident_data=accident_data.replace([\"白色\",\"黑色\",\"红色\",\"银色\",\"灰色\",\"黄色\",\"绿色\",\"蓝色\",\"白色\",\"黑 \",\"北\",\" 银\",\"BAI\",\"棕色\"],\n",
    "                      [\"白\",\"黑\",\"红\",\"银\",\"灰\",\"黄\",\"绿\",\"蓝\",\"白\",\"黑\",\"白\",\"银\",\"白\",\"灰\"],[\"carcolor1\",\"carcolor2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap2={\"白\":1,\"黑\":2,\"银\":3,\"红\":4,\"蓝\":5,\"黄\":6,\"绿\":7,\"灰\":8,\"其他\":0}\n",
    "accident_data=accident_data.replace([\"其他\",\"白\",\"黑\",\"银\",\"红\",\"蓝\",\"黄\",\"绿\",\"灰\"],[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\"],[\"carcolor1\",\"carcolor2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_type(df,cols):\n",
    "    for col in cols:\n",
    "        \n",
    "        df=df.withColumn(col+\"Tmp\",df[col].cast(pyspark.sql.types.IntegerType())).\\\n",
    "                                    drop(col).withColumnRenamed(col+\"Tmp\",col)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accident_data=turn_type(accident_data,\"carcolor2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_map={\"多云\",\"阵雨\",\"阴\",\"小雨\",\"晴\",\"中雨\",\"雷阵雨\",\"大雨\",\"暴雨\",\"冻雨\"}\n",
    "accident_data=accident_data.replace([\"多云\",\"阵雨\",\"阴\",\"小雨\",\"晴\",\"中雨\",\"雷阵雨\",\"大雨\",\"暴雨\",\"冻雨\"],\n",
    "                                    [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"8\"],[\"weather1\",\"weather2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accident_data=turn_type(accident_data,\"weather1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_age(age):\n",
    "    if age:\n",
    "        age=float(age)\n",
    "        if (age<30):\n",
    "            return 0\n",
    "        elif (age>29 and age<45) :\n",
    "            return 1\n",
    "        elif (age>44 and age<60):\n",
    "            return 2\n",
    "        else :\n",
    "            return 3\n",
    "    else:\n",
    "        return 3\n",
    "sparkF=functions.udf(get_age,pyspark.sql.types.IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "accident_data=accident_data.withColumn(\"driver1_age_category\",sparkF(accident_data.driver1_age))\n",
    "accident_data=accident_data.withColumn(\"driver2_age_category\",sparkF(accident_data.driver2_age))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_year(year):\n",
    "    if year:\n",
    "        year=float(year)\n",
    "        if (year<4):\n",
    "            return 0\n",
    "        elif (year>3 and year<11) :\n",
    "            return 1\n",
    "        elif (year>10 and year<21):\n",
    "            return 2\n",
    "        else :\n",
    "            return 3\n",
    "    else:\n",
    "        return 0\n",
    "sparkY=functions.udf(get_year,pyspark.sql.types.IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "accident_data=accident_data.withColumn(\"driver1_year_category\",sparkY(accident_data.driver1_years))\n",
    "accident_data=accident_data.withColumn(\"driver2_year_category\",sparkY(accident_data.driver2_years))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------------------+\n|driver1_year_category|driver2_year_category|\n+---------------------+---------------------+\n|                    0|                    0|\n|                    0|                    0|\n|                    0|                    0|\n|                    1|                    1|\n|                    1|                    1|\n|                    1|                    1|\n|                    1|                    1|\n|                    1|                    1|\n|                    0|                    0|\n|                    3|                    3|\n|                    0|                    0|\n|                    2|                    2|\n|                    2|                    2|\n|                    0|                    0|\n|                    0|                    0|\n|                    0|                    0|\n|                    1|                    1|\n|                    2|                    2|\n|                    1|                    1|\n|                    1|                    1|\n+---------------------+---------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "accident_data.select([\"driver1_year_category\",\"driver2_year_category\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "district_map={\"南明区\",\"云岩区\",\"乌当区\",\"花溪区\",\"观山湖区\",\"白云区\"}\n",
    "accident_data=accident_data.replace([\"南明区\",\"云岩区\",\"乌当区\",\"花溪区\",\"观山湖区\",\"白云区\"],\n",
    "                                    [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\"],\"district\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "accident_data=accident_data.replace([\"东北风\",\"南风\",\"东南风\",\"东风\"],[\"1\",\"2\",\"3\",\"4\"],[\"wind1\",\"wind2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "accident_data=accident_data.replace([\"负全部责任\",\"负同等责任\"],[\"1\",\"0\"],\"driver1responsibility\")\n",
    "accident_data=accident_data.replace([\"不负责任\",\"负同等责任\"],[\"1\",\"0\"],\"driver2responsibility\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accident_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "accident_data=accident_data.fillna(\"0\",\"clpp1\")\n",
    "clpp1=accident_data.groupby(\"clpp1\").count()\n",
    "clpp2=accident_data.groupby(\"clpp2\").count()\n",
    "clpp1=clpp1.sort(\"count\",ascending=False)\n",
    "clpp2=clpp2.sort(\"count\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "clpp1=clpp1.select(\"clpp1\")\n",
    "clpp2=clpp2.select(\"clpp2\")\n",
    "clpp1_list=clpp1.take(clpp1.count())\n",
    "clpp2_list=clpp2.take(clpp2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clpp_list(clpp_list,start,end,c):\n",
    "    clpp=[]\n",
    "    for i in range(start,end):\n",
    "        clpp.append(clpp_list[i].asDict()[c])\n",
    "    return clpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "clpp1_list_divided=list(range(4))\n",
    "clpp1_list_divided[0]=get_clpp_list(clpp1_list,0,1,\"clpp1\")\n",
    "clpp1_list_divided[1]=get_clpp_list(clpp1_list,1,10,\"clpp1\")\n",
    "clpp1_list_divided[2]=get_clpp_list(clpp1_list,10,40,\"clpp1\")\n",
    "clpp1_list_divided[3]=get_clpp_list(clpp1_list,40,len(clpp1_list),\"clpp1\")\n",
    "\n",
    "clpp2_list_divided=list(range(4))\n",
    "clpp2_list_divided[0]=get_clpp_list(clpp2_list,0,1,\"clpp2\")\n",
    "clpp2_list_divided[1]=get_clpp_list(clpp2_list,1,10,\"clpp2\")\n",
    "clpp2_list_divided[2]=get_clpp_list(clpp2_list,10,40,\"clpp2\")\n",
    "clpp2_list_divided[3]=get_clpp_list(clpp2_list,40,len(clpp2_list),\"clpp2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clpp_num(clpp,clpp_list_divided):\n",
    "    for i in range(4):\n",
    "        if clpp in clpp_list_divided[i]:\n",
    "            return i\n",
    "        else :continue\n",
    "# def get_clpp_num2(clpp):\n",
    "#     for i in range(5):\n",
    "#         if clpp in clpp2_list_divided[i]:\n",
    "#             return i\n",
    "#         else :continue\n",
    "make_clpp1_function=functions.udf(lambda c :get_clpp_num(c,clpp1_list_divided),pyspark.sql.types.IntegerType())\n",
    "make_clpp2_function=functions.udf(lambda c :get_clpp_num(c,clpp2_list_divided),pyspark.sql.types.IntegerType())\n",
    "\n",
    "# sparkC1=functions.udf(get_clpp_num,pyspark.sql.types.IntegerType())\n",
    "# sparkC2=functions.udf(get_clpp_num,pyspark.sql.types.IntegerType())\n",
    "accident_data=accident_data.withColumn(\"clpp1_category\",make_clpp1_function(accident_data.clpp1))\n",
    "accident_data=accident_data.withColumn(\"clpp2_category\",make_clpp2_function(accident_data.clpp1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n|clpp1_category|clpp2_category|\n+--------------+--------------+\n|             2|             2|\n|             1|             1|\n|             1|             1|\n|             2|             2|\n|             3|             3|\n|             3|             3|\n|             1|             1|\n|             2|             2|\n|             2|             2|\n|             1|             1|\n|             1|             1|\n|             1|             2|\n|             2|             2|\n|             0|             0|\n|             3|             3|\n|             1|             1|\n|             2|             2|\n|             1|             1|\n|             0|             0|\n|             3|             3|\n+--------------+--------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "accident_data.select([\"clpp1_category\",\"clpp2_category\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n|clpp1_category|clpp2_category|\n+--------------+--------------+\n|             2|             2|\n|             1|             1|\n|             1|             1|\n|             2|             2|\n|             3|             3|\n|             3|             3|\n|             1|             1|\n|             2|             2|\n|             2|             2|\n|             1|             1|\n|             1|             1|\n|             1|             2|\n|             2|             2|\n|             0|             0|\n|             3|             3|\n|             1|             1|\n|             2|             2|\n|             1|             1|\n|             0|             0|\n|             3|             3|\n+--------------+--------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "accident_data.select([\"clpp1_category\",\"clpp2_category\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "jxmc1=accident_data.groupby(\"jxmc1\").count()\n",
    "jxmc2=accident_data.groupby(\"jxmc2\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "jxmc1=jxmc1.sort(\"count\",ascending=False)\n",
    "jxmc2=jxmc2.sort(\"count\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "jxmc1=jxmc1.select(\"jxmc1\")\n",
    "jxmc2=jxmc2.select(\"jxmc2\")\n",
    "jxmc1_list=jxmc1.take(jxmc1.count())\n",
    "jxmc2_list=jxmc2.take(jxmc2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jxmc_list(jxmc_list,start,end,c):\n",
    "    jxmc=[]\n",
    "    for i in range(start,end):\n",
    "        jxmc.append(jxmc_list[i].asDict()[c])\n",
    "    return jxmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "jxmc1_list_divided=list(range(5))\n",
    "jxmc2_list_divided=list(range(5))\n",
    "jxmc1_list_divided[0]=get_jxmc_list(jxmc1_list,0,1,\"jxmc1\")\n",
    "jxmc1_list_divided[1]=get_jxmc_list(jxmc1_list,1,2,\"jxmc1\")\n",
    "jxmc1_list_divided[2]=get_jxmc_list(jxmc1_list,2,10,\"jxmc1\")\n",
    "jxmc1_list_divided[3]=get_jxmc_list(jxmc1_list,10,40,\"jxmc1\")\n",
    "jxmc1_list_divided[4]=get_jxmc_list(jxmc1_list,40,len(jxmc1_list),\"jxmc1\")\n",
    "jxmc2_list_divided[0]=get_jxmc_list(jxmc2_list,0,1,\"jxmc2\")\n",
    "jxmc2_list_divided[1]=get_jxmc_list(jxmc2_list,1,2,\"jxmc2\")\n",
    "jxmc2_list_divided[2]=get_jxmc_list(jxmc2_list,2,10,\"jxmc2\")\n",
    "jxmc2_list_divided[3]=get_jxmc_list(jxmc2_list,10,40,\"jxmc2\")\n",
    "jxmc2_list_divided[4]=get_jxmc_list(jxmc2_list,40,len(jxmc2_list),\"jxmc2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jxmc_num1(jxmc):\n",
    "    for i in range(5):\n",
    "        if jxmc in jxmc1_list_divided[i]:\n",
    "            return i\n",
    "        else :continue\n",
    "def get_jxmc_num2(jxmc):\n",
    "    for i in range(5):\n",
    "        if jxmc in jxmc2_list_divided[i]:\n",
    "            return i\n",
    "        else :continue\n",
    "sparkJ1=functions.udf(get_jxmc_num1,pyspark.sql.types.IntegerType())\n",
    "sparkJ2=functions.udf(get_jxmc_num1,pyspark.sql.types.IntegerType())\n",
    "accident_data=accident_data.withColumn(\"jxmc1_category\",sparkJ1(accident_data.jxmc1))\n",
    "accident_data=accident_data.withColumn(\"jxmc2_category\",sparkJ2(accident_data.jxmc1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n|jxmc1_category|jxmc2_category|\n+--------------+--------------+\n|             1|             1|\n|             4|             4|\n|             3|             3|\n|             2|             2|\n|             0|             0|\n|             3|             3|\n|             0|             0|\n|             1|             1|\n|             2|             2|\n|             3|             3|\n|             2|             2|\n|             1|             1|\n|             0|             0|\n|             2|             2|\n|             0|             0|\n|             1|             1|\n|             1|             1|\n|             0|             0|\n|             0|             0|\n|             2|             2|\n+--------------+--------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "accident_data.select([\"jxmc1_category\",\"jxmc2_category\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n|driver1_age_category|\n+--------------------+\n|                   1|\n|                   1|\n|                   2|\n|                   2|\n|                   1|\n|                   1|\n|                   1|\n|                   3|\n|                   1|\n|                   1|\n|                   2|\n|                   3|\n|                   1|\n|                   1|\n|                   1|\n|                   1|\n|                   1|\n|                   1|\n|                   1|\n|                   2|\n+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "accident_data.select(\"driver1_age_category\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a=accident_data.select(pyspark.sql.functions.mean(accident_data.driver1_years))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s=pyspark.sql.functions.mean(accident_data.driver1_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "accident_data=accident_data.withColumn(\"temperature\",(accident_data.temperature_min+accident_data.temperature_max)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|temperature|\n+-----------+\n|        9.0|\n|        7.0|\n|        7.0|\n|        4.5|\n|        4.0|\n|        3.0|\n|        5.0|\n|        5.0|\n|        5.0|\n|        5.0|\n|        5.0|\n|        5.0|\n|        5.5|\n|        5.5|\n|        5.5|\n|        5.5|\n|        5.5|\n|        7.0|\n|        6.5|\n|        6.5|\n+-----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "accident_data.select(\"temperature\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- accidenttime: string (nullable = true)\n |-- accidentaddr: string (nullable = true)\n |-- driver1fault: string (nullable = true)\n |-- driver1responsibility: string (nullable = true)\n |-- driver2responsibility: string (nullable = true)\n |-- sex1: string (nullable = true)\n |-- platenumber1: string (nullable = true)\n |-- carcolor1: string (nullable = true)\n |-- sex2: string (nullable = true)\n |-- platenumber2: string (nullable = true)\n |-- carcolor2: string (nullable = true)\n |-- jxmc1: string (nullable = true)\n |-- jxmc2: string (nullable = true)\n |-- cclzrq1: string (nullable = true)\n |-- cclzrq2: string (nullable = true)\n |-- clpp1: string (nullable = false)\n |-- clpp2: string (nullable = true)\n |-- driver1license: string (nullable = true)\n |-- driver2license: string (nullable = true)\n |-- brith1: string (nullable = true)\n |-- brith2: string (nullable = true)\n |-- accident_month: string (nullable = true)\n |-- accident_quarter: string (nullable = true)\n |-- accident_weekday: string (nullable = true)\n |-- accident_day: string (nullable = true)\n |-- accident_hour: string (nullable = true)\n |-- accident_minute: string (nullable = true)\n |-- is_province1: string (nullable = true)\n |-- is_province2: string (nullable = true)\n |-- is_city1: string (nullable = true)\n |-- is_city2: string (nullable = true)\n |-- is_private: string (nullable = true)\n |-- end_number1: string (nullable = true)\n |-- end_number2: string (nullable = true)\n |-- is_private1: string (nullable = true)\n |-- is_private2: string (nullable = true)\n |-- is_driver1_city: string (nullable = true)\n |-- is_driver2_city: string (nullable = true)\n |-- is_driver1_province: string (nullable = true)\n |-- is_driver2_province: string (nullable = true)\n |-- driver1_days: string (nullable = true)\n |-- driver2_days: string (nullable = true)\n |-- driver1_years: string (nullable = true)\n |-- driver2_years: string (nullable = true)\n |-- driver1_age: string (nullable = true)\n |-- driver2_age: string (nullable = true)\n |-- difference_age: string (nullable = true)\n |-- clcpp1: string (nullable = true)\n |-- clcpp2: string (nullable = true)\n |-- temperature_min: string (nullable = true)\n |-- temperature_max: string (nullable = true)\n |-- weather1: string (nullable = true)\n |-- weather2: string (nullable = true)\n |-- wind1: string (nullable = true)\n |-- wind2: string (nullable = true)\n |-- diff_temperature: string (nullable = true)\n |-- district: string (nullable = true)\n |-- lng: string (nullable = true)\n |-- lat: string (nullable = true)\n |-- jszh_x: string (nullable = true)\n |-- fine_x: string (nullable = true)\n |-- score_x: string (nullable = true)\n |-- maxtime_x: string (nullable = true)\n |-- xfcount_x: string (nullable = true)\n |-- wfxw_x: string (nullable = true)\n |-- jszh_y: string (nullable = true)\n |-- fine_y: string (nullable = true)\n |-- score_y: string (nullable = true)\n |-- maxtime_y: string (nullable = true)\n |-- xfcount_y: string (nullable = true)\n |-- wfxw_y: string (nullable = true)\n |-- driver1_age_category: integer (nullable = true)\n |-- driver2_age_category: integer (nullable = true)\n |-- driver1_year_category: integer (nullable = true)\n |-- driver2_year_category: integer (nullable = true)\n |-- clpp1_category: integer (nullable = true)\n |-- clpp2_category: integer (nullable = true)\n |-- jxmc1_category: integer (nullable = true)\n |-- jxmc2_category: integer (nullable = true)\n |-- temperature: double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "accident_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=[\"driver1fault\",\"sex1\",\"carcolor1\",\"clpp1\",\"jxmc1\",\n",
    "      \"sex2\",\"carcolor2\",\"clpp2\",\"jxmc2\",\n",
    "      \"accident_month\",\"accident_quarter\",\"accident_weekday\",\"accident_day\",\"accident_hour\",\"accident_minute\",\n",
    "      \"is_province1\",\"is_city1\",\"is_driver1_city\",\"is_driver1_province\",\n",
    "      \"is_province2\",\"is_city2\",\"is_driver2_city\",\"is_driver2_province\",\n",
    "      \"weather1\",\"weather2\",\"wind1\",\"wind2\",\"district\",\"lng\",\"lat\",\n",
    "       \"fine_x\", \"score_x\", \"maxtime_x\", \"xfcount_x\", \"wfxw_x\",\"fine_y\", \"score_y\", \"maxtime_y\", \"xfcount_y\", \"wfxw_y\"]\n",
    "accident_data=turn_type(accident_data,cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "accident_data=accident_data.fillna(0,[\"fine_x\", \"score_x\",\"xfcount_x\", \"wfxw_x\",\n",
    "                                      \"fine_y\", \"score_y\",\"xfcount_y\", \"wfxw_y\",\"district\",\"carcolor1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- accidenttime: string (nullable = true)\n |-- accidentaddr: string (nullable = true)\n |-- driver1responsibility: string (nullable = true)\n |-- driver2responsibility: string (nullable = true)\n |-- platenumber1: string (nullable = true)\n |-- platenumber2: string (nullable = true)\n |-- cclzrq1: string (nullable = true)\n |-- cclzrq2: string (nullable = true)\n |-- driver1license: string (nullable = true)\n |-- driver2license: string (nullable = true)\n |-- brith1: string (nullable = true)\n |-- brith2: string (nullable = true)\n |-- is_private: string (nullable = true)\n |-- end_number1: string (nullable = true)\n |-- end_number2: string (nullable = true)\n |-- is_private1: string (nullable = true)\n |-- is_private2: string (nullable = true)\n |-- driver1_days: string (nullable = true)\n |-- driver2_days: string (nullable = true)\n |-- driver1_years: string (nullable = true)\n |-- driver2_years: string (nullable = true)\n |-- driver1_age: string (nullable = true)\n |-- driver2_age: string (nullable = true)\n |-- difference_age: string (nullable = true)\n |-- clcpp1: string (nullable = true)\n |-- clcpp2: string (nullable = true)\n |-- temperature_min: string (nullable = true)\n |-- temperature_max: string (nullable = true)\n |-- diff_temperature: string (nullable = true)\n |-- jszh_x: string (nullable = true)\n |-- jszh_y: string (nullable = true)\n |-- driver1_age_category: integer (nullable = true)\n |-- driver2_age_category: integer (nullable = true)\n |-- driver1_year_category: integer (nullable = true)\n |-- driver2_year_category: integer (nullable = true)\n |-- clpp1_category: integer (nullable = true)\n |-- clpp2_category: integer (nullable = true)\n |-- jxmc1_category: integer (nullable = true)\n |-- jxmc2_category: integer (nullable = true)\n |-- temperature: double (nullable = true)\n |-- driver1fault: integer (nullable = true)\n |-- sex1: integer (nullable = true)\n |-- carcolor1: integer (nullable = true)\n |-- clpp1: integer (nullable = true)\n |-- jxmc1: integer (nullable = true)\n |-- sex2: integer (nullable = true)\n |-- carcolor2: integer (nullable = true)\n |-- clpp2: integer (nullable = true)\n |-- jxmc2: integer (nullable = true)\n |-- accident_month: integer (nullable = true)\n |-- accident_quarter: integer (nullable = true)\n |-- accident_weekday: integer (nullable = true)\n |-- accident_day: integer (nullable = true)\n |-- accident_hour: integer (nullable = true)\n |-- accident_minute: integer (nullable = true)\n |-- is_province1: integer (nullable = true)\n |-- is_city1: integer (nullable = true)\n |-- is_driver1_city: integer (nullable = true)\n |-- is_driver1_province: integer (nullable = true)\n |-- is_province2: integer (nullable = true)\n |-- is_city2: integer (nullable = true)\n |-- is_driver2_city: integer (nullable = true)\n |-- is_driver2_province: integer (nullable = true)\n |-- weather1: integer (nullable = true)\n |-- weather2: integer (nullable = true)\n |-- wind1: integer (nullable = true)\n |-- wind2: integer (nullable = true)\n |-- district: integer (nullable = true)\n |-- lng: integer (nullable = true)\n |-- lat: integer (nullable = true)\n |-- fine_x: integer (nullable = true)\n |-- score_x: integer (nullable = true)\n |-- maxtime_x: integer (nullable = true)\n |-- xfcount_x: integer (nullable = true)\n |-- wfxw_x: integer (nullable = true)\n |-- fine_y: integer (nullable = true)\n |-- score_y: integer (nullable = true)\n |-- maxtime_y: integer (nullable = true)\n |-- xfcount_y: integer (nullable = true)\n |-- wfxw_y: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "accident_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver1=accident_data.select([\"driver1fault\",\"sex1\",\"carcolor1\",\"clpp1_category\",\n",
    "             \"jxmc1_category\",\"accident_month\",\"accident_weekday\",\n",
    "             \"accident_day\",\"accident_hour\",\"accident_minute\",\"is_province1\",\n",
    "             \"is_city1\",\"is_driver1_city\",\"is_driver1_province\",\n",
    "             \"temperature\",\n",
    "             \"weather1\",\"weather2\",\"wind1\",\"wind2\",\"district\",\"lng\",\"lat\",\n",
    "             \"driver1_age_category\",\"driver1_year_category\",\n",
    "              \"fine_x\", \"score_x\", \"maxtime_x\", \"xfcount_x\", \"wfxw_x\"\n",
    "             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- driver1fault: integer (nullable = true)\n |-- sex1: integer (nullable = true)\n |-- carcolor1: integer (nullable = true)\n |-- clpp1_category: integer (nullable = true)\n |-- jxmc1_category: integer (nullable = true)\n |-- accident_month: integer (nullable = true)\n |-- accident_weekday: integer (nullable = true)\n |-- accident_day: integer (nullable = true)\n |-- accident_hour: integer (nullable = true)\n |-- accident_minute: integer (nullable = true)\n |-- is_province1: integer (nullable = true)\n |-- is_city1: integer (nullable = true)\n |-- is_driver1_city: integer (nullable = true)\n |-- is_driver1_province: integer (nullable = true)\n |-- temperature: double (nullable = true)\n |-- weather1: integer (nullable = true)\n |-- weather2: integer (nullable = true)\n |-- wind1: integer (nullable = true)\n |-- wind2: integer (nullable = true)\n |-- district: integer (nullable = true)\n |-- lng: integer (nullable = true)\n |-- lat: integer (nullable = true)\n |-- driver1_age_category: integer (nullable = true)\n |-- driver1_year_category: integer (nullable = true)\n |-- fine_x: integer (nullable = true)\n |-- score_x: integer (nullable = true)\n |-- maxtime_x: integer (nullable = true)\n |-- xfcount_x: integer (nullable = true)\n |-- wfxw_x: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "driver1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class RandomForestClassifier in module pyspark.ml.classification:\n\nclass RandomForestClassifier(pyspark.ml.wrapper.JavaEstimator, pyspark.ml.param.shared.HasFeaturesCol, pyspark.ml.param.shared.HasLabelCol, pyspark.ml.param.shared.HasPredictionCol, pyspark.ml.param.shared.HasSeed, pyspark.ml.param.shared.HasRawPredictionCol, pyspark.ml.param.shared.HasProbabilityCol, pyspark.ml.regression.RandomForestParams, TreeClassifierParams, pyspark.ml.param.shared.HasCheckpointInterval, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n |  `Random Forest <http://en.wikipedia.org/wiki/Random_forest>`_\n |  learning algorithm for classification.\n |  It supports both binary and multiclass labels, as well as both continuous and categorical\n |  features.\n |  \n |  >>> import numpy\n |  >>> from numpy import allclose\n |  >>> from pyspark.ml.linalg import Vectors\n |  >>> from pyspark.ml.feature import StringIndexer\n |  >>> df = spark.createDataFrame([\n |  ...     (1.0, Vectors.dense(1.0)),\n |  ...     (0.0, Vectors.sparse(1, [], []))], [\"label\", \"features\"])\n |  >>> stringIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexed\")\n |  >>> si_model = stringIndexer.fit(df)\n |  >>> td = si_model.transform(df)\n |  >>> rf = RandomForestClassifier(numTrees=3, maxDepth=2, labelCol=\"indexed\", seed=42)\n |  >>> model = rf.fit(td)\n |  >>> model.featureImportances\n |  SparseVector(1, {0: 1.0})\n |  >>> allclose(model.treeWeights, [1.0, 1.0, 1.0])\n |  True\n |  >>> test0 = spark.createDataFrame([(Vectors.dense(-1.0),)], [\"features\"])\n |  >>> result = model.transform(test0).head()\n |  >>> result.prediction\n |  0.0\n |  >>> numpy.argmax(result.probability)\n |  0\n |  >>> numpy.argmax(result.rawPrediction)\n |  0\n |  >>> test1 = spark.createDataFrame([(Vectors.sparse(1, [0], [1.0]),)], [\"features\"])\n |  >>> model.transform(test1).head().prediction\n |  1.0\n |  >>> model.trees\n |  [DecisionTreeClassificationModel (uid=...) of depth..., DecisionTreeClassificationModel...]\n |  >>> rfc_path = temp_path + \"/rfc\"\n |  >>> rf.save(rfc_path)\n |  >>> rf2 = RandomForestClassifier.load(rfc_path)\n |  >>> rf2.getNumTrees()\n |  3\n |  >>> model_path = temp_path + \"/rfc_model\"\n |  >>> model.save(model_path)\n |  >>> model2 = RandomForestClassificationModel.load(model_path)\n |  >>> model.featureImportances == model2.featureImportances\n |  True\n |  \n |  .. versionadded:: 1.4.0\n |  \n |  Method resolution order:\n |      RandomForestClassifier\n |      pyspark.ml.wrapper.JavaEstimator\n |      pyspark.ml.wrapper.JavaParams\n |      pyspark.ml.wrapper.JavaWrapper\n |      pyspark.ml.base.Estimator\n |      pyspark.ml.param.shared.HasFeaturesCol\n |      pyspark.ml.param.shared.HasLabelCol\n |      pyspark.ml.param.shared.HasPredictionCol\n |      pyspark.ml.param.shared.HasSeed\n |      pyspark.ml.param.shared.HasRawPredictionCol\n |      pyspark.ml.param.shared.HasProbabilityCol\n |      pyspark.ml.regression.RandomForestParams\n |      pyspark.ml.regression.TreeEnsembleParams\n |      pyspark.ml.param.shared.DecisionTreeParams\n |      TreeClassifierParams\n |      pyspark.ml.param.shared.HasCheckpointInterval\n |      pyspark.ml.param.Params\n |      pyspark.ml.util.Identifiable\n |      pyspark.ml.util.JavaMLWritable\n |      pyspark.ml.util.MLWritable\n |      pyspark.ml.util.JavaMLReadable\n |      pyspark.ml.util.MLReadable\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __init__(self, featuresCol='features', labelCol='label', predictionCol='prediction', probabilityCol='probability', rawPredictionCol='rawPrediction', maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0, maxMemoryInMB=256, cacheNodeIds=False, checkpointInterval=10, impurity='gini', numTrees=20, featureSubsetStrategy='auto', seed=None, subsamplingRate=1.0)\n |      __init__(self, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",                  probabilityCol=\"probability\", rawPredictionCol=\"rawPrediction\",                  maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0,                  maxMemoryInMB=256, cacheNodeIds=False, checkpointInterval=10, impurity=\"gini\",                  numTrees=20, featureSubsetStrategy=\"auto\", seed=None, subsamplingRate=1.0)\n |  \n |  setParams(self, featuresCol='features', labelCol='label', predictionCol='prediction', probabilityCol='probability', rawPredictionCol='rawPrediction', maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0, maxMemoryInMB=256, cacheNodeIds=False, checkpointInterval=10, seed=None, impurity='gini', numTrees=20, featureSubsetStrategy='auto', subsamplingRate=1.0)\n |      setParams(self, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",                  probabilityCol=\"probability\", rawPredictionCol=\"rawPrediction\",                   maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0,                   maxMemoryInMB=256, cacheNodeIds=False, checkpointInterval=10, seed=None,                   impurity=\"gini\", numTrees=20, featureSubsetStrategy=\"auto\", subsamplingRate=1.0)\n |      Sets params for linear classification.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.wrapper.JavaEstimator:\n |  \n |  __metaclass__ = <class 'abc.ABCMeta'>\n |      Metaclass for defining Abstract Base Classes (ABCs).\n |      \n |      Use this metaclass to create an ABC.  An ABC can be subclassed\n |      directly, and then acts as a mix-in class.  You can also register\n |      unrelated concrete classes (even built-in classes) and unrelated\n |      ABCs as 'virtual subclasses' -- these and their descendants will\n |      be considered subclasses of the registering ABC by the built-in\n |      issubclass() function, but the registering ABC won't show up in\n |      their MRO (Method Resolution Order) nor will method\n |      implementations defined by the registering ABC be callable (not\n |      even via super()).\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.wrapper.JavaParams:\n |  \n |  __del__(self)\n |  \n |  copy(self, extra=None)\n |      Creates a copy of this instance with the same uid and some\n |      extra params. This implementation first calls Params.copy and\n |      then make a copy of the companion Java pipeline component with\n |      extra params. So both the Python wrapper and the Java pipeline\n |      component get copied.\n |      \n |      :param extra: Extra parameters to copy to the new instance\n |      :return: Copy of this instance\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.base.Estimator:\n |  \n |  fit(self, dataset, params=None)\n |      Fits a model to the input dataset with optional parameters.\n |      \n |      :param dataset: input dataset, which is an instance of :py:class:`pyspark.sql.DataFrame`\n |      :param params: an optional param map that overrides embedded params. If a list/tuple of\n |                     param maps is given, this calls fit on each param map and returns a list of\n |                     models.\n |      :returns: fitted model(s)\n |      \n |      .. versionadded:: 1.3.0\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.shared.HasFeaturesCol:\n |  \n |  getFeaturesCol(self)\n |      Gets the value of featuresCol or its default value.\n |  \n |  setFeaturesCol(self, value)\n |      Sets the value of :py:attr:`featuresCol`.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.param.shared.HasFeaturesCol:\n |  \n |  featuresCol = Param(parent='undefined', name='featuresCol', doc='featu...\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.shared.HasLabelCol:\n |  \n |  getLabelCol(self)\n |      Gets the value of labelCol or its default value.\n |  \n |  setLabelCol(self, value)\n |      Sets the value of :py:attr:`labelCol`.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.param.shared.HasLabelCol:\n |  \n |  labelCol = Param(parent='undefined', name='labelCol', doc='label colum...\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.shared.HasPredictionCol:\n |  \n |  getPredictionCol(self)\n |      Gets the value of predictionCol or its default value.\n |  \n |  setPredictionCol(self, value)\n |      Sets the value of :py:attr:`predictionCol`.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.param.shared.HasPredictionCol:\n |  \n |  predictionCol = Param(parent='undefined', name='predictionCol', doc='p...\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.shared.HasSeed:\n |  \n |  getSeed(self)\n |      Gets the value of seed or its default value.\n |  \n |  setSeed(self, value)\n |      Sets the value of :py:attr:`seed`.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.param.shared.HasSeed:\n |  \n |  seed = Param(parent='undefined', name='seed', doc='random seed.')\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n |  \n |  getRawPredictionCol(self)\n |      Gets the value of rawPredictionCol or its default value.\n |  \n |  setRawPredictionCol(self, value)\n |      Sets the value of :py:attr:`rawPredictionCol`.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n |  \n |  rawPredictionCol = Param(parent='undefined', name='rawPredictionCol......\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.shared.HasProbabilityCol:\n |  \n |  getProbabilityCol(self)\n |      Gets the value of probabilityCol or its default value.\n |  \n |  setProbabilityCol(self, value)\n |      Sets the value of :py:attr:`probabilityCol`.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.param.shared.HasProbabilityCol:\n |  \n |  probabilityCol = Param(parent='undefined', name='probabilityCol',...at...\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.regression.RandomForestParams:\n |  \n |  getFeatureSubsetStrategy(self)\n |      Gets the value of featureSubsetStrategy or its default value.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  getNumTrees(self)\n |      Gets the value of numTrees or its default value.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  setFeatureSubsetStrategy(self, value)\n |      Sets the value of :py:attr:`featureSubsetStrategy`.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  setNumTrees(self, value)\n |      Sets the value of :py:attr:`numTrees`.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.regression.RandomForestParams:\n |  \n |  featureSubsetStrategy = Param(parent='undefined', name='featureSubsetS...\n |  \n |  numTrees = Param(parent='undefined', name='numTrees', doc='Number of t...\n |  \n |  supportedFeatureSubsetStrategies = ['auto', 'all', 'onethird', 'sqrt',...\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.regression.TreeEnsembleParams:\n |  \n |  getSubsamplingRate(self)\n |      Gets the value of subsamplingRate or its default value.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  setSubsamplingRate(self, value)\n |      Sets the value of :py:attr:`subsamplingRate`.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.regression.TreeEnsembleParams:\n |  \n |  subsamplingRate = Param(parent='undefined', name='subsamplingRate'...r...\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.shared.DecisionTreeParams:\n |  \n |  getCacheNodeIds(self)\n |      Gets the value of cacheNodeIds or its default value.\n |  \n |  getMaxBins(self)\n |      Gets the value of maxBins or its default value.\n |  \n |  getMaxDepth(self)\n |      Gets the value of maxDepth or its default value.\n |  \n |  getMaxMemoryInMB(self)\n |      Gets the value of maxMemoryInMB or its default value.\n |  \n |  getMinInfoGain(self)\n |      Gets the value of minInfoGain or its default value.\n |  \n |  getMinInstancesPerNode(self)\n |      Gets the value of minInstancesPerNode or its default value.\n |  \n |  setCacheNodeIds(self, value)\n |      Sets the value of :py:attr:`cacheNodeIds`.\n |  \n |  setMaxBins(self, value)\n |      Sets the value of :py:attr:`maxBins`.\n |  \n |  setMaxDepth(self, value)\n |      Sets the value of :py:attr:`maxDepth`.\n |  \n |  setMaxMemoryInMB(self, value)\n |      Sets the value of :py:attr:`maxMemoryInMB`.\n |  \n |  setMinInfoGain(self, value)\n |      Sets the value of :py:attr:`minInfoGain`.\n |  \n |  setMinInstancesPerNode(self, value)\n |      Sets the value of :py:attr:`minInstancesPerNode`.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.param.shared.DecisionTreeParams:\n |  \n |  cacheNodeIds = Param(parent='undefined', name='cacheNodeIds', d...ed o...\n |  \n |  maxBins = Param(parent='undefined', name='maxBins', doc='M...mber of c...\n |  \n |  maxDepth = Param(parent='undefined', name='maxDepth', doc='...; depth ...\n |  \n |  maxMemoryInMB = Param(parent='undefined', name='maxMemoryInMB', ...ati...\n |  \n |  minInfoGain = Param(parent='undefined', name='minInfoGain', do...in fo...\n |  \n |  minInstancesPerNode = Param(parent='undefined', name='minInstancesPerN...\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from TreeClassifierParams:\n |  \n |  getImpurity(self)\n |      Gets the value of impurity or its default value.\n |      \n |      .. versionadded:: 1.6.0\n |  \n |  setImpurity(self, value)\n |      Sets the value of :py:attr:`impurity`.\n |      \n |      .. versionadded:: 1.6.0\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from TreeClassifierParams:\n |  \n |  impurity = Param(parent='undefined', name='impurity', doc='...-insensi...\n |  \n |  supportedImpurities = ['entropy', 'gini']\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.shared.HasCheckpointInterval:\n |  \n |  getCheckpointInterval(self)\n |      Gets the value of checkpointInterval or its default value.\n |  \n |  setCheckpointInterval(self, value)\n |      Sets the value of :py:attr:`checkpointInterval`.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pyspark.ml.param.shared.HasCheckpointInterval:\n |  \n |  checkpointInterval = Param(parent='undefined', name='checkpointInterv....\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.param.Params:\n |  \n |  explainParam(self, param)\n |      Explains a single param and returns its name, doc, and optional\n |      default value and user-supplied value in a string.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  explainParams(self)\n |      Returns the documentation of all params with their optionally\n |      default values and user-supplied values.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  extractParamMap(self, extra=None)\n |      Extracts the embedded default param values and user-supplied\n |      values, and then merges them with extra values from input into\n |      a flat param map, where the latter value is used if there exist\n |      conflicts, i.e., with ordering: default param values <\n |      user-supplied values < extra.\n |      \n |      :param extra: extra param values\n |      :return: merged param map\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  getOrDefault(self, param)\n |      Gets the value of a param in the user-supplied param map or its\n |      default value. Raises an error if neither is set.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  getParam(self, paramName)\n |      Gets a param by its name.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  hasDefault(self, param)\n |      Checks whether a param has a default value.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  hasParam(self, paramName)\n |      Tests whether this instance contains a param with a given\n |      (string) name.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  isDefined(self, param)\n |      Checks whether a param is explicitly set by user or has\n |      a default value.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  isSet(self, param)\n |      Checks whether a param is explicitly set by user.\n |      \n |      .. versionadded:: 1.4.0\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from pyspark.ml.param.Params:\n |  \n |  params\n |      Returns all params ordered by name. The default implementation\n |      uses :py:func:`dir` to get all attributes of type\n |      :py:class:`Param`.\n |      \n |      .. versionadded:: 1.3.0\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.util.Identifiable:\n |  \n |  __repr__(self)\n |      Return repr(self).\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.util.JavaMLWritable:\n |  \n |  write(self)\n |      Returns an MLWriter instance for this ML instance.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pyspark.ml.util.MLWritable:\n |  \n |  save(self, path)\n |      Save this ML instance to the given path, a shortcut of `write().save(path)`.\n |  \n |  ----------------------------------------------------------------------\n |  Class methods inherited from pyspark.ml.util.JavaMLReadable:\n |  \n |  read() from builtins.type\n |      Returns an MLReader instance for this class.\n |  \n |  ----------------------------------------------------------------------\n |  Class methods inherited from pyspark.ml.util.MLReadable:\n |  \n |  load(path) from builtins.type\n |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n\n"
     ]
    }
   ],
   "source": [
    "help(RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}